[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Crop_yield_CW",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "data/precipitation/readme.html",
    "href": "data/precipitation/readme.html",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "This data package contains the data that powers the chart “Annual precipitation” on the Our World in Data website. It was downloaded on May 03, 2025.\n\n\nA filtered subset of the full data was downloaded. The following filters were applied:\n\n\n\nThe high level structure of the CSV file is that each row is an observation for an entity (usually a country or region) and a timepoint (usually a year).\nThe first two columns in the CSV file are “Entity” and “Code”. “Entity” is the name of the entity (e.g. “United States”). “Code” is the OWID internal entity code that we use if the entity is a country or region. For normal countries, this is the same as the iso alpha-3 code of the entity (e.g. “USA”) - for non-standard countries like historical countries these are custom codes.\nThe third column is either “Year” or “Day”. If the data is annual, this is “Year” and contains only the year as an integer. If the column is “Day”, the column contains a date string in the form “YYYY-MM-DD”.\nThe final column is the data column, which is the time series that powers the chart. If the CSV data is downloaded using the “full data” option, then the column corresponds to the time series below. If the CSV data is downloaded using the “only selected data visible in the chart” option then the data column is transformed depending on the chart type and thus the association with the time series might not be as straightforward.\n\n\n\nThe .metadata.json file contains metadata about the data package. The “charts” key contains information to recreate the chart, like the title, subtitle etc.. The “columns” key contains information about each of the columns in the csv, like the unit, timespan covered, citation for the data etc..\n\n\n\nOur World in Data is almost never the original producer of the data - almost all of the data we use has been compiled by others. If you want to re-use data, it is your responsibility to ensure that you adhere to the sources’ license and to credit them correctly. Please note that a single time series may have more than one source - e.g. when we stich together data from different time periods by different producers or when we calculate per capita metrics using population data from a second source.\n\n\n\n\n\n\nTotal annual precipitation—rain and snow—calculated as the sum of daily averages, reported as the depth of water falling to Earth’s surface, excluding fog and dew. Last updated: January 7, 2025\nNext update: July 2025\nDate range: 1940–2024\nUnit: millimeters\n\n\n\n\nIf you have limited space (e.g. in data visualizations), you can use this abbreviated in-line citation:\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data\n\n\n\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data. “Annual precipitation” [dataset]. Contains modified Copernicus Climate Change Service information, “ERA5 monthly averaged data on single levels from 1940 to present 2” [original data]. Source: Contains modified Copernicus Climate Change Service information (2025) – with major processing by Our World In Data\n\n\n\n\nThis parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This parameter is accumulated over a particular time period which depends on the data extracted. For the monthly averaged reanalysis and the monthly averaged ensemble members, the accumulation period is 1 day. For the monthly averaged reanalysis by hour of day, the accumulation period is 1 hour and for the monthly averaged ensemble members by hour of day, the accumulation period is 3 hours. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box.\n\n\n\n\n\nRetrieved on: 2025-01-07\nRetrieved from: https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=overview\n\n\n\n\nInitially, the dataset is provided with specific coordinates in terms of longitude and latitude. To tailor this data to each country, we use geographical boundaries as defined by the World Bank. The method involves trimming the precipitation dataset to match the exact geographical shape of each country. To correct for potential distortions caused by projecting the Earth’s curved surface onto a flat map, we apply a latitude-based weighting. This step is essential for maintaining accuracy, particularly in high-latitude regions where distortion is more pronounced. The result of this process is a latitude-weighted average precipitation for each nation.\nIt’s important to note, however, that due to the resolution constraints of the Copernicus dataset, this methodology might not be as effective for countries with very small landmasses. In such cases, the process may not yield reliable data.\nThe derived precipitation for each country is calculated based on administrative borders, encompassing all land surface types within these areas. As a result, precipitation over oceans and seas is not included in these averages, keeping the data focused on terrestrial environments.\nGlobal precipitation averages and anomalies, however, are calculated over both land and ocean surfaces.\nThe precipitation anomaly is calculated by comparing the average precipitation of a specific time period (e.g., a particular year or month) to the average surface precipitation of the same period from 1991 to 2020.\nWhen calculating anomalies for each country, the total precipitation of a given year or month is compared to the 1991-2020 average precipitation for that specific country.\nThe reason for using the 1991-2020 period as the reference mean is that it is the standard reference period used by our data source, the Copernicus Climate Change Service. This period is also adopted by the UK Met Office. This approach ensures consistency in identifying climate variations over time."
  },
  {
    "objectID": "data/precipitation/readme.html#csv-structure",
    "href": "data/precipitation/readme.html#csv-structure",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "The high level structure of the CSV file is that each row is an observation for an entity (usually a country or region) and a timepoint (usually a year).\nThe first two columns in the CSV file are “Entity” and “Code”. “Entity” is the name of the entity (e.g. “United States”). “Code” is the OWID internal entity code that we use if the entity is a country or region. For normal countries, this is the same as the iso alpha-3 code of the entity (e.g. “USA”) - for non-standard countries like historical countries these are custom codes.\nThe third column is either “Year” or “Day”. If the data is annual, this is “Year” and contains only the year as an integer. If the column is “Day”, the column contains a date string in the form “YYYY-MM-DD”.\nThe final column is the data column, which is the time series that powers the chart. If the CSV data is downloaded using the “full data” option, then the column corresponds to the time series below. If the CSV data is downloaded using the “only selected data visible in the chart” option then the data column is transformed depending on the chart type and thus the association with the time series might not be as straightforward."
  },
  {
    "objectID": "data/precipitation/readme.html#metadata.json-structure",
    "href": "data/precipitation/readme.html#metadata.json-structure",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "The .metadata.json file contains metadata about the data package. The “charts” key contains information to recreate the chart, like the title, subtitle etc.. The “columns” key contains information about each of the columns in the csv, like the unit, timespan covered, citation for the data etc.."
  },
  {
    "objectID": "data/precipitation/readme.html#about-the-data",
    "href": "data/precipitation/readme.html#about-the-data",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "Our World in Data is almost never the original producer of the data - almost all of the data we use has been compiled by others. If you want to re-use data, it is your responsibility to ensure that you adhere to the sources’ license and to credit them correctly. Please note that a single time series may have more than one source - e.g. when we stich together data from different time periods by different producers or when we calculate per capita metrics using population data from a second source."
  },
  {
    "objectID": "data/precipitation/readme.html#annual-precipitation",
    "href": "data/precipitation/readme.html#annual-precipitation",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "Total annual precipitation—rain and snow—calculated as the sum of daily averages, reported as the depth of water falling to Earth’s surface, excluding fog and dew. Last updated: January 7, 2025\nNext update: July 2025\nDate range: 1940–2024\nUnit: millimeters\n\n\n\n\nIf you have limited space (e.g. in data visualizations), you can use this abbreviated in-line citation:\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data\n\n\n\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data. “Annual precipitation” [dataset]. Contains modified Copernicus Climate Change Service information, “ERA5 monthly averaged data on single levels from 1940 to present 2” [original data]. Source: Contains modified Copernicus Climate Change Service information (2025) – with major processing by Our World In Data\n\n\n\n\nThis parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This parameter is accumulated over a particular time period which depends on the data extracted. For the monthly averaged reanalysis and the monthly averaged ensemble members, the accumulation period is 1 day. For the monthly averaged reanalysis by hour of day, the accumulation period is 1 hour and for the monthly averaged ensemble members by hour of day, the accumulation period is 3 hours. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box.\n\n\n\n\n\nRetrieved on: 2025-01-07\nRetrieved from: https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=overview\n\n\n\n\nInitially, the dataset is provided with specific coordinates in terms of longitude and latitude. To tailor this data to each country, we use geographical boundaries as defined by the World Bank. The method involves trimming the precipitation dataset to match the exact geographical shape of each country. To correct for potential distortions caused by projecting the Earth’s curved surface onto a flat map, we apply a latitude-based weighting. This step is essential for maintaining accuracy, particularly in high-latitude regions where distortion is more pronounced. The result of this process is a latitude-weighted average precipitation for each nation.\nIt’s important to note, however, that due to the resolution constraints of the Copernicus dataset, this methodology might not be as effective for countries with very small landmasses. In such cases, the process may not yield reliable data.\nThe derived precipitation for each country is calculated based on administrative borders, encompassing all land surface types within these areas. As a result, precipitation over oceans and seas is not included in these averages, keeping the data focused on terrestrial environments.\nGlobal precipitation averages and anomalies, however, are calculated over both land and ocean surfaces.\nThe precipitation anomaly is calculated by comparing the average precipitation of a specific time period (e.g., a particular year or month) to the average surface precipitation of the same period from 1991 to 2020.\nWhen calculating anomalies for each country, the total precipitation of a given year or month is compared to the 1991-2020 average precipitation for that specific country.\nThe reason for using the 1991-2020 period as the reference mean is that it is the standard reference period used by our data source, the Copernicus Climate Change Service. This period is also adopted by the UK Met Office. This approach ensures consistency in identifying climate variations over time."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "100-eda.html",
    "href": "100-eda.html",
    "title": "Model Fit",
    "section": "",
    "text": "Goals\nThe project shall aim to study the effect of precipitation and temperature on crop yield, and predict the effect of rising temperature on crop yield within the UK.\n\n\nData\n\nThe data on crop yield was obtained from the Food and Agriculture Organization of the United Nations, which includes various data on crops and livestock products from across the world. The project shall mainly focus on data for the United Kingdom.\nThe data for precipitation was obtained from Out World in Data, and records the average precipitation for countries on an annual basis.\nThe data for temperature was obtained from the World Bank Group. It contains historic data on the mean surface air temperature.\n\nWe shall mainly be looking at 3 crops: wheat, barley, and rapeseed, which represent the 3 most popular crops grown in the UK (GOV.UK. ‘Agricultural Land Use in United Kingdom at 1 June 2024’. Accessed 3 May 2025.).\n There appears to be a general trend of better crop yield over time. The yield of all 3 crops appears to be highly correlated, suggesting that they have common causes. One thing to note here is that the yield for rapeseed production is recorded as constant for the first 5 years of the data, and almost constant for the next 3 years. This seems unlikely, given the rest of the data, and will probably negatively impact the model fit for rapeseed yield.\n \nWe can see clearly that for each crop the yield is positively correlated with annual precipitation and average temperature."
  },
  {
    "objectID": "analyses/100-eda.html",
    "href": "analyses/100-eda.html",
    "title": "100-eda",
    "section": "",
    "text": "Loading required package: ggplot2\n\n\nLoading required package: data.table\n\n\nLoading required package: here\n\n\nhere() starts at C:/Users/harry/Projects/Imperial/Data_Science/Crop_yield_CW\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::between()     masks data.table::between()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::first()       masks data.table::first()\n✖ lubridate::hour()    masks data.table::hour()\n✖ lubridate::isoweek() masks data.table::isoweek()\n✖ dplyr::lag()         masks stats::lag()\n✖ dplyr::last()        masks data.table::last()\n✖ lubridate::mday()    masks data.table::mday()\n✖ lubridate::minute()  masks data.table::minute()\n✖ lubridate::month()   masks data.table::month()\n✖ lubridate::quarter() masks data.table::quarter()\n✖ lubridate::second()  masks data.table::second()\n✖ purrr::transpose()   masks data.table::transpose()\n✖ lubridate::wday()    masks data.table::wday()\n✖ lubridate::week()    masks data.table::week()\n✖ lubridate::yday()    masks data.table::yday()\n✖ lubridate::year()    masks data.table::year()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\n\n\ncrop_and_precipitation_data &lt;- merge(crop_data, precipitation_data, by=\"Year\")\n\np &lt;- ggplot(crop_and_precipitation_data, \n       aes(x=Annual.precipitation, \n           y=Yield, \n           group=Item, \n           colour=Item)) +\n  geom_point() +\n  labs(x = \"Annual Precipitation (mm)\") +\n  theme_minimal()\n\nggsave(file = file.path(out.dir, '100-eda', 'precipitation_yield.png'), p)\n\nSaving 7 x 5 in image\n\np\n\n\n\n\n\n\n\n\n\nfile &lt;- file.path(data.dir, 'temp', 'observed-annual-average.csv')\ndt &lt;- as.data.table(read.csv(file))\n\ntemp_data &lt;- dt %&gt;%\n  rename(Year = Category,\n         Avg.Temp = Annual.Mean) %&gt;%\n  select(-X5.yr.smooth)\n\n\ncrop_and_temp_data &lt;- merge(crop_data, temp_data, by=\"Year\")\n\np &lt;- ggplot(crop_and_temp_data, \n       aes(x=Avg.Temp, \n           y=Yield, \n           group=Item, \n           colour=Item)) +\n  geom_point() +\n  labs(x = \"Average temperature (Celsius)\") +\n  theme_minimal()\n\nggsave(file = file.path(out.dir, '100-eda', 'temp_yield.png'), p)\n\nSaving 7 x 5 in image\n\np"
  },
  {
    "objectID": "200-model-fit.html",
    "href": "200-model-fit.html",
    "title": "Model Fit",
    "section": "",
    "text": "Model\nI will fit a simple model for crop yield against precipitation and average annual temperature. Let \\(y_{i,t}\\) be the crop yield of crop \\(i\\) at year \\(t\\),\\(T_t\\) the average temperature of year \\(t\\), \\(P_t\\) the average precipitation at year \\(t\\). The model will assume that \\(y_{i,t}\\) follows a lognormal distribution. \\[\n\\begin{align}\ny_{i,t} &\\sim \\text{LogNormal}(\\mu_{i,t}, \\sigma_i^2) \\\\\n\\mu_{i,t} &= \\alpha_i + \\beta_i T_t + \\gamma_i P_t \\\\\n\\end{align}\n\\]\nThe model will be fit using a Bayesian model, using the Stan software draw samples from the posterior. Given that precipitation and temperature operate at very different scales, I have chosen to standardise the predictors to help with numerical stability. Let \\(T'_t\\) and \\(P'_t\\) be the standardised temperature and precipitation, respectively. Then: \\[\n\\mu_{i,t} = \\alpha'_i + \\beta'_i T'_t + \\gamma'_i P'_t \\\\\n\\] is equivalent to the previous formulation when \\[\n\\begin{align}\n\\alpha_i &= \\alpha'_i - \\beta'_i \\frac{\\mu_T}{\\sigma_T} - \\gamma'_i \\frac{\\mu_P}{\\sigma_P} \\\\\n\\beta_i &= \\frac{\\beta'_i}{\\sigma_T} \\\\\n\\gamma_i &= \\frac{\\gamma'_i}{\\sigma_P} \\\\\n\\end{align}\n\\] where \\(\\mu_T\\), \\(\\mu_P\\), \\(\\sigma_T\\), \\(\\sigma_P\\) are the means and standard deviations for temperature and precipitation, respectively.\nFinally, here is the full model specification: \\[\n\\begin{align}\ny_{i,t} &\\sim \\text{LogNormal}(\\mu_{i,t}, \\sigma_i^2) \\\\\n\\mu_{i,t} &= \\alpha'_i + \\beta'_i T'_t + \\gamma'_i P'_t \\\\\n\\alpha'_i &\\sim \\text{Normal}(0, 10^2) \\\\\n\\beta'_i &\\sim \\text{Normal}(0,1) \\\\\n\\gamma'_i &\\sim \\text{Normal}(0,1) \\\\\n\\sigma_i &\\sim \\text{Half-Normal}(0,1) \\\\\n\\end{align}\n\\]\n\n\nModel Check\nTo confirm that the model does indeed fit the data well, I performed a posterior predictive check. To do this, I generated Monte Carlo samples from the posterior predictive distribution \\[\np(y^*_j \\, | \\, y) = \\int p(y^*_j \\, | \\, \\theta) p(\\theta \\, | \\, y) d\\theta\n\\] After this, an empirical posterior predictive interval can be calculated for each observation based on the Monte Carlo samples.\n From the plot we can see that the model matches the data reasonably well, and roughly 95% of all data points fall within the posterior predictive interval.\n\n\nSummary\nSince we are using a log-normal model, the median yield \\(y_{i,t}\\) is given by \\(\\exp(\\mu_{i,t})\\). We can view the marginal effects of 1 standard unit increase in temperature by looking at the distribution of \\(\\exp(\\beta_i) \\, | \\, y\\). This represent the multiplicative effect of 1 standard unit increase in temperature on the median yield. The graph below shows the 95% confidence interval for the effect of temperature, for each crop.\n We can see that wheat is the most sensitive to temperature, while rapeseed is the least sensitive. As seen in exploratory data analyses, increases in average annual temperature correlates to higher yields.\nDoing the same for precipitation, we get:  Here, we see that precipitation has around the same effect on all 3 crops, around a 5-8% increase in median crop yield."
  },
  {
    "objectID": "data/raw/precipitation/readme.html",
    "href": "data/raw/precipitation/readme.html",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "This data package contains the data that powers the chart “Annual precipitation” on the Our World in Data website. It was downloaded on May 03, 2025.\n\n\nA filtered subset of the full data was downloaded. The following filters were applied:\n\n\n\nThe high level structure of the CSV file is that each row is an observation for an entity (usually a country or region) and a timepoint (usually a year).\nThe first two columns in the CSV file are “Entity” and “Code”. “Entity” is the name of the entity (e.g. “United States”). “Code” is the OWID internal entity code that we use if the entity is a country or region. For normal countries, this is the same as the iso alpha-3 code of the entity (e.g. “USA”) - for non-standard countries like historical countries these are custom codes.\nThe third column is either “Year” or “Day”. If the data is annual, this is “Year” and contains only the year as an integer. If the column is “Day”, the column contains a date string in the form “YYYY-MM-DD”.\nThe final column is the data column, which is the time series that powers the chart. If the CSV data is downloaded using the “full data” option, then the column corresponds to the time series below. If the CSV data is downloaded using the “only selected data visible in the chart” option then the data column is transformed depending on the chart type and thus the association with the time series might not be as straightforward.\n\n\n\nThe .metadata.json file contains metadata about the data package. The “charts” key contains information to recreate the chart, like the title, subtitle etc.. The “columns” key contains information about each of the columns in the csv, like the unit, timespan covered, citation for the data etc..\n\n\n\nOur World in Data is almost never the original producer of the data - almost all of the data we use has been compiled by others. If you want to re-use data, it is your responsibility to ensure that you adhere to the sources’ license and to credit them correctly. Please note that a single time series may have more than one source - e.g. when we stich together data from different time periods by different producers or when we calculate per capita metrics using population data from a second source.\n\n\n\n\n\n\nTotal annual precipitation—rain and snow—calculated as the sum of daily averages, reported as the depth of water falling to Earth’s surface, excluding fog and dew. Last updated: January 7, 2025\nNext update: July 2025\nDate range: 1940–2024\nUnit: millimeters\n\n\n\n\nIf you have limited space (e.g. in data visualizations), you can use this abbreviated in-line citation:\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data\n\n\n\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data. “Annual precipitation” [dataset]. Contains modified Copernicus Climate Change Service information, “ERA5 monthly averaged data on single levels from 1940 to present 2” [original data]. Source: Contains modified Copernicus Climate Change Service information (2025) – with major processing by Our World In Data\n\n\n\n\nThis parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This parameter is accumulated over a particular time period which depends on the data extracted. For the monthly averaged reanalysis and the monthly averaged ensemble members, the accumulation period is 1 day. For the monthly averaged reanalysis by hour of day, the accumulation period is 1 hour and for the monthly averaged ensemble members by hour of day, the accumulation period is 3 hours. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box.\n\n\n\n\n\nRetrieved on: 2025-01-07\nRetrieved from: https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=overview\n\n\n\n\nInitially, the dataset is provided with specific coordinates in terms of longitude and latitude. To tailor this data to each country, we use geographical boundaries as defined by the World Bank. The method involves trimming the precipitation dataset to match the exact geographical shape of each country. To correct for potential distortions caused by projecting the Earth’s curved surface onto a flat map, we apply a latitude-based weighting. This step is essential for maintaining accuracy, particularly in high-latitude regions where distortion is more pronounced. The result of this process is a latitude-weighted average precipitation for each nation.\nIt’s important to note, however, that due to the resolution constraints of the Copernicus dataset, this methodology might not be as effective for countries with very small landmasses. In such cases, the process may not yield reliable data.\nThe derived precipitation for each country is calculated based on administrative borders, encompassing all land surface types within these areas. As a result, precipitation over oceans and seas is not included in these averages, keeping the data focused on terrestrial environments.\nGlobal precipitation averages and anomalies, however, are calculated over both land and ocean surfaces.\nThe precipitation anomaly is calculated by comparing the average precipitation of a specific time period (e.g., a particular year or month) to the average surface precipitation of the same period from 1991 to 2020.\nWhen calculating anomalies for each country, the total precipitation of a given year or month is compared to the 1991-2020 average precipitation for that specific country.\nThe reason for using the 1991-2020 period as the reference mean is that it is the standard reference period used by our data source, the Copernicus Climate Change Service. This period is also adopted by the UK Met Office. This approach ensures consistency in identifying climate variations over time."
  },
  {
    "objectID": "data/raw/precipitation/readme.html#csv-structure",
    "href": "data/raw/precipitation/readme.html#csv-structure",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "The high level structure of the CSV file is that each row is an observation for an entity (usually a country or region) and a timepoint (usually a year).\nThe first two columns in the CSV file are “Entity” and “Code”. “Entity” is the name of the entity (e.g. “United States”). “Code” is the OWID internal entity code that we use if the entity is a country or region. For normal countries, this is the same as the iso alpha-3 code of the entity (e.g. “USA”) - for non-standard countries like historical countries these are custom codes.\nThe third column is either “Year” or “Day”. If the data is annual, this is “Year” and contains only the year as an integer. If the column is “Day”, the column contains a date string in the form “YYYY-MM-DD”.\nThe final column is the data column, which is the time series that powers the chart. If the CSV data is downloaded using the “full data” option, then the column corresponds to the time series below. If the CSV data is downloaded using the “only selected data visible in the chart” option then the data column is transformed depending on the chart type and thus the association with the time series might not be as straightforward."
  },
  {
    "objectID": "data/raw/precipitation/readme.html#metadata.json-structure",
    "href": "data/raw/precipitation/readme.html#metadata.json-structure",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "The .metadata.json file contains metadata about the data package. The “charts” key contains information to recreate the chart, like the title, subtitle etc.. The “columns” key contains information about each of the columns in the csv, like the unit, timespan covered, citation for the data etc.."
  },
  {
    "objectID": "data/raw/precipitation/readme.html#about-the-data",
    "href": "data/raw/precipitation/readme.html#about-the-data",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "Our World in Data is almost never the original producer of the data - almost all of the data we use has been compiled by others. If you want to re-use data, it is your responsibility to ensure that you adhere to the sources’ license and to credit them correctly. Please note that a single time series may have more than one source - e.g. when we stich together data from different time periods by different producers or when we calculate per capita metrics using population data from a second source."
  },
  {
    "objectID": "data/raw/precipitation/readme.html#annual-precipitation",
    "href": "data/raw/precipitation/readme.html#annual-precipitation",
    "title": "Annual precipitation - Data package",
    "section": "",
    "text": "Total annual precipitation—rain and snow—calculated as the sum of daily averages, reported as the depth of water falling to Earth’s surface, excluding fog and dew. Last updated: January 7, 2025\nNext update: July 2025\nDate range: 1940–2024\nUnit: millimeters\n\n\n\n\nIf you have limited space (e.g. in data visualizations), you can use this abbreviated in-line citation:\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data\n\n\n\nContains modified Copernicus Climate Change Service information (2025) – with major processing by Our World in Data. “Annual precipitation” [dataset]. Contains modified Copernicus Climate Change Service information, “ERA5 monthly averaged data on single levels from 1940 to present 2” [original data]. Source: Contains modified Copernicus Climate Change Service information (2025) – with major processing by Our World In Data\n\n\n\n\nThis parameter is the accumulated liquid and frozen water, comprising rain and snow, that falls to the Earth’s surface. It is the sum of large-scale precipitation and convective precipitation. Large-scale precipitation is generated by the cloud scheme in the ECMWF Integrated Forecasting System (IFS). The cloud scheme represents the formation and dissipation of clouds and large-scale precipitation due to changes in atmospheric quantities (such as pressure, temperature and moisture) predicted directly by the IFS at spatial scales of the grid box or larger. Convective precipitation is generated by the convection scheme in the IFS, which represents convection at spatial scales smaller than the grid box. This parameter does not include fog, dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the Earth. This parameter is accumulated over a particular time period which depends on the data extracted. For the monthly averaged reanalysis and the monthly averaged ensemble members, the accumulation period is 1 day. For the monthly averaged reanalysis by hour of day, the accumulation period is 1 hour and for the monthly averaged ensemble members by hour of day, the accumulation period is 3 hours. The units of this parameter are depth in metres of water equivalent. It is the depth the water would have if it were spread evenly over the grid box. Care should be taken when comparing model parameters with observations, because observations are often local to a particular point in space and time, rather than representing averages over a model grid box.\n\n\n\n\n\nRetrieved on: 2025-01-07\nRetrieved from: https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means?tab=overview\n\n\n\n\nInitially, the dataset is provided with specific coordinates in terms of longitude and latitude. To tailor this data to each country, we use geographical boundaries as defined by the World Bank. The method involves trimming the precipitation dataset to match the exact geographical shape of each country. To correct for potential distortions caused by projecting the Earth’s curved surface onto a flat map, we apply a latitude-based weighting. This step is essential for maintaining accuracy, particularly in high-latitude regions where distortion is more pronounced. The result of this process is a latitude-weighted average precipitation for each nation.\nIt’s important to note, however, that due to the resolution constraints of the Copernicus dataset, this methodology might not be as effective for countries with very small landmasses. In such cases, the process may not yield reliable data.\nThe derived precipitation for each country is calculated based on administrative borders, encompassing all land surface types within these areas. As a result, precipitation over oceans and seas is not included in these averages, keeping the data focused on terrestrial environments.\nGlobal precipitation averages and anomalies, however, are calculated over both land and ocean surfaces.\nThe precipitation anomaly is calculated by comparing the average precipitation of a specific time period (e.g., a particular year or month) to the average surface precipitation of the same period from 1991 to 2020.\nWhen calculating anomalies for each country, the total precipitation of a given year or month is compared to the 1991-2020 average precipitation for that specific country.\nThe reason for using the 1991-2020 period as the reference mean is that it is the standard reference period used by our data source, the Copernicus Climate Change Service. This period is also adopted by the UK Met Office. This approach ensures consistency in identifying climate variations over time."
  },
  {
    "objectID": "300-forecast.html",
    "href": "300-forecast.html",
    "title": "Forecasting",
    "section": "",
    "text": "Temperature Forecast Model\nTo investigate the effect of increasing temperature on crop yield, I first fitted a model for the average annual temperature data. I assumed a quadratic trend in the temperature, with some non-parametric fluctuations modelled by a Gaussian process with an RBF kernel, with scale parameter \\(\\alpha\\) and lengthscale \\(\\rho\\). The time \\(t\\) has been linearly transformed to fall within the interval \\([-1, 1]\\). \\[\n\\begin{align}\nT_t &\\sim \\text{Normal}(\\mu_t, \\sigma^2) \\\\\n\\mu_t &= \\beta_0 + \\beta_1 t + \\beta_2 t^2 + f(t) \\\\\nf &\\sim \\text{GP}(\\alpha, \\rho) \\\\\n\\alpha & \\sim \\text{Half-Cauchy}(0,1) \\\\\n\\rho & \\sim \\text{Inv-Gamma}(5, 1) \\\\\n\\beta_0 & \\sim \\text{Normal}(0, 10^2) \\\\\n\\beta_1, \\, \\beta_2 & \\sim \\text{Normal}(0, 1) \\\\\n\\sigma & \\sim \\text{Half-Normal}(0,1)\n\\end{align}\n\\] The goal is to be able to use this model to forecast the temperature for the next 10 years based on historic data. In order to efficiently sample from the posterior of the Gaussian process, I have used a fast Hilbert Space approximation (Riutort-Mayol, G., Bürkner, PC., Andersen, M.R. et al. 2023). Some of the stan code used to implement this was copied from the MATH70073 - Advanced Bayesian Methods course notes.\n The model provides an adequate fit to the data, and captures the inherent uncertainty underlying the data. There is a clear non-stationary increasing trend in the last 50 or so years.\n\n\nImpact on Crop Yield\nUsing the forecast values for temperature, and combined with the previous model for crop yield, we can predict the impact of increasing temperature on yield. For this purposes we shall assume a constant annual precipitation based on the average over the last 10 years. The forecast is based on the log-normal model’s median, \\(\\exp(\\hat{\\mu}_{i,t})\\).\n  \nThe forecast for wheat and barley appears to be somewhat reasonable given historic data. The forecast for rapeseed appears to be overly confident in its predictions. As a sanity check, forecast appears to be consistent with the previous results, which showed that wheat is the most sensitive to increases in temperature, and rapeseed the least sensitive."
  }
]